In main_centralized, get_centralized_model_and_trainer is called located in:
In trainers __init__.py file, factory methods are defined to create the centralized model + trainer.
Here, we need to switch between models.
This method in turn calls get_centralized_model, which is renamed for it's specific modality
each modality like image retrieval has it's own factory methods to create a base model with the correct params.

These factory methods use the classes CentralizedModel, or MetaTransformerBase.
These classes are made per modality.
CentralizedModel inherets MetaTransformerBase.
Each CentralizedModel class has its own specific implementation for the specific modality.
In the init of centralizedModel, load all of the pre-trained weights.
The Client and Server models then use this centralized base model to init their models.


Changes to be made:er



Plan:

Create an ae_vit.py class implementing class AE_ViT_Base. We can then use roughly the same logic of the models.py classes
First create a
Need to ensure that the AE is actually trained with the rest of the model.


Training:



Did the previous work not use early saving of the model?

To-do:
- implement AE training method
- create multi run structure
- add option to change LR for AE.


Structure of single full training:
- get_ae_pretrainer


single main_2_stage file runs:
- Check if AE needs to be pre-trained. If a flag is set, the AE is re-trained regardless of existing AE.
- Each saved AE has a corresponding config file. This is used to load the AE with the correct params.
- If AE needs to be pre-trained, call get_ae_pretrainer to get the AE pretrainer object.
-