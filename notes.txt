In main_centralized, get_centralized_model_and_trainer is called located in:
In trainers __init__.py file, factory methods are defined to create the centralized model + trainer.
Here, we need to switch between models.
This method in turn calls get_centralized_model, which is renamed for it's specific modality
each modality like image retrieval has it's own factory methods to create a base model with the correct params.

These factory methods use the classes CentralizedModel, or MetaTransformerBase.
These classes are made per modality.
CentralizedModel inherets MetaTransformerBase.
Each CentralizedModel class has its own specific implementation for the specific modality.
In the init of centralizedModel, load all of the pre-trained weights.
The Client and Server models then use this centralized base model to init their models.


Changes to be made:er



Plan:

Create an ae_vit.py class implementing class AE_ViT_Base. We can then use roughly the same logic of the models.py classes
First create a
Need to ensure that the AE is actually trained with the rest of the model.


Training:



Did the previous work not use early saving of the model?

To-do:
- implement AE training method
- create multi run structure
- add option to change LR for AE.


Structure of single full training:
- get_centralized_ae_pretrainer. We give this file the dataloader, initialized AE already
-


single main_2_stage file runs:
- Check if AE needs to be pre-trained. If a flag is set, the AE is re-trained regardless of existing AE.
- Each saved AE has a corresponding config file. This is used to load the AE with the correct params.
- If AE needs to be pre-trained, call get_ae_pretrainer to get the AE pretrainer object.
-



main_centralized_2_stage.py: loads data, calls methods to pre-train AE, then calls methods to train the full model. Handles the two-stage training process.
                    If additional steps have to be made, this may need to be delegated

- get_centralized_ae_model_and_pretrainer:
    - calls get_ae_model(global_args, use_pre_trained, model_path=None) to get the AE model. Based on parser args, it decides on a newly initialized AE or using an existing AE.
        A file path needs to be passed as a parameter. This method also verifies that the AE is compatible.
    - calls get_centralized_ae_pretrainer to get either a central AE pre-trainer or a client-side pre_trainer. This pre-trainer is passed as a function, which is then used in main


need 3 steps:
- load the pre-trained model (VisionTransformerBase)
- pass this model to AE pre-trainer
- then get the AEVisionTransformer by passing VisionTransformerBase and the AE.


Currently, we do not use validation loss to save the best model!. See if early stopping is necessary.

TODO:
- Need to fix that the activation to train the AE are not recalculated each epoch.
- Need to implement the MPSL training method
- Need to fix AE training, seems like results are suboptimal.
- Need to implement plotting / results gathering code.
- Add option to only pre-train AEs

- Add more datasets.