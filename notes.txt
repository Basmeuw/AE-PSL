In main_centralized, get_centralized_model_and_trainer is called located in:
In trainers __init__.py file, factory methods are defined to create the centralized model + trainer.
Here, we need to switch between models.
This method in turn calls get_centralized_model, which is renamed for it's specific modality
each modality like image retrieval has it's own factory methods to create a base model with the correct params.

These factory methods use the classes CentralizedModel, or MetaTransformerBase.
These classes are made per modality.
CentralizedModel inherets MetaTransformerBase.
Each CentralizedModel class has its own specific implementation for the specific modality.
In the init of centralizedModel, load all of the pre-trained weights.
The Client and Server models then use this centralized base model to init their models.


Changes to be made:er



Plan:

Create an ae_vit.py class implementing class AE_ViT_Base. We can then use roughly the same logic of the models.py classes
First create a
Need to ensure that the AE is actually trained with the rest of the model.


Training:



Did the previous work not use early saving of the model?

To-do:
- implement AE training method
- create multi run structure
- add option to change LR for AE.


Structure of single full training:
- get_centralized_ae_pretrainer. We give this file the dataloader, initialized AE already
-


single main_2_stage file runs:
- Check if AE needs to be pre-trained. If a flag is set, the AE is re-trained regardless of existing AE.
- Each saved AE has a corresponding config file. This is used to load the AE with the correct params.
- If AE needs to be pre-trained, call get_ae_pretrainer to get the AE pretrainer object.
-



main_centralized_2_stage.py: loads data, calls methods to pre-train AE, then calls methods to train the full model. Handles the two-stage training process.
                    If additional steps have to be made, this may need to be delegated

- get_centralized_ae_model_and_pretrainer:
    - calls get_ae_model(global_args, use_pre_trained, model_path=None) to get the AE model. Based on parser args, it decides on a newly initialized AE or using an existing AE.
        A file path needs to be passed as a parameter. This method also verifies that the AE is compatible.
    - calls get_centralized_ae_pretrainer to get either a central AE pre-trainer or a client-side pre_trainer. This pre-trainer is passed as a function, which is then used in main


need 3 steps:
- load the pre-trained model (VisionTransformerBase)
- pass this model to AE pre-trainer
- then get the AEVisionTransformer by passing VisionTransformerBase and the AE.


Currently, we do not use validation loss to save the best model!. See if early stopping is necessary.


Centralized training:
Load train and test dataset
Load base model
We check in main if the dataset is the same, if not, load the seperate dataset, and pass it as param to external function
if the dataset is the same, print using downstream dataset (option 2)
The external function creates the dataloaders.


Distributed training
Load train and test dataset
Load base model
Option 1: pre-train single AE on a centralized dataset.

Option 2: pre-train AE on downstream dataset. Each client pre-trains its own AE.
Here we either need to aggregate the decoders after training, or keep a per-client decoder on the server-side.
We also need to take care of the non-iid distribution.


TODO:
- figure out why AE pre-training is slow
- Need to fix AE training, seems like results are suboptimal.

- Need to implement plotting / results gathering code.
- Check why 'Files downloaded and verified' appears multiple times during data loading.
- Add more datasets.
- Need to adapt results to per-client structure
- Give the pretrained AEs unique names

- Fix on edge AE training vs normal training
- Need to write out all possible scenario's for AE dataset loading, for centralized and normal training.
- Cache the activations for AE training, if allowed (so need the same dataset and split layer to be reused)
-

- potentially use the ViT MAE from facebook? Generalizes better